{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa179e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# =================== HUGGINGFACE TOKEN ===================\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"**\"\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"]) # Hidden\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "model_id = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "input_file = \"./Arxiv-papers2000/Final-50.xlsx\"\n",
    "output_file = \"llm_judgments_qwen2.5.json\"\n",
    "\n",
    "# =================== PROMPT FUNCTION ===================\n",
    "def build_prompt(table: str, gen_para: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert in content alignment. You are given a Table (<Table>) and a Paragraph generated by an LLM (<LLM-generated>).\n",
    "\n",
    "Your task is to evaluate the generated paragraph against the table content across the following four dimensions:\n",
    "\n",
    "D1 – Factual Correctness:\n",
    "Is the LLM-generated paragraph factually accurate based on the table content?\n",
    "Score: Binary (0 or 1)\n",
    "1 = All factual claims in the paragraph are correct based on the table.\n",
    "0 = The paragraph includes any incorrect or misstated information.\n",
    "\n",
    "D2 – Reasoning Depth:\n",
    "Does the paragraph demonstrate reasoning beyond simply restating facts?\n",
    "Score: Binary (0 or 1)\n",
    "1 = Shows inference, interpretation, or synthesis.\n",
    "0 = Only repeats or paraphrases information from the table.\n",
    "\n",
    "D3 – Reasoning Type:\n",
    "What is the primary type of reasoning demonstrated?\n",
    "Score: One of the following categories only\n",
    "Comparative, No Reasoning\n",
    "\n",
    "D4 – Relevance:\n",
    "Does the paragraph stay focused and include only relevant, meaningful information from the table?\n",
    "Score: Binary (0 or 1)\n",
    "1 = Shows relevance with the table's contents.\n",
    "0 = doesn't Show any relevance with the table's content\n",
    "\n",
    "Return only your scores in the following format (no explanation):\n",
    "\n",
    "Factual_correctness: <your-answer>,\n",
    "Reasoning_depth: <your-answer>,\n",
    "Reasoning_type: <your-answer>,\n",
    "Relevance: <your-answer>\n",
    "\n",
    "<Table>\n",
    "{table}\n",
    "\n",
    "<LLM-generated>\n",
    "{gen_para}\n",
    "\"\"\"\n",
    "\n",
    "# =================== PARSE FUNCTION ===================\n",
    "def extract_structured_scores(response):\n",
    "    result = {}\n",
    "    for line in response.splitlines():\n",
    "        if \":\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\":\", 1)\n",
    "        k = k.strip()\n",
    "        v = v.strip(\"`,.;: \")\n",
    "        if v:\n",
    "            result[k] = v\n",
    "    return result\n",
    "\n",
    "# =================== LOAD MODEL ===================\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# =================== LOAD DATA ===================\n",
    "df = pd.read_excel(input_file, sheet_name=\"Sheet1\", skiprows=1)\n",
    "df.columns = df.iloc[0]         \n",
    "df = df[1:]                     # Delete Header \n",
    "df = df.reset_index(drop=True)                   \n",
    "df = df.iloc[:50]\n",
    "\n",
    "cols = [\n",
    "    \"source_models\", \"filename\", \"table_label\", \"table_caption\", \"table_content\", \"referencing_paragraphs_cleaned\", \"_drop\",\n",
    "    \"llama_factual\", \"llama_Reasoningdepth\", \"llama_Reasoningtype\", \"llama_relevance\", \"llama3.3_70b_sentence\", \"llama_score\",\n",
    "    \"mistral_score\", \"mistral_factual\", \"mistral_Reasoningdepth\", \"mistral_Reasoningtype\", \"mistral_relevance\", \"Mistral24B_sentence\",\n",
    "    \"gemma_score\", \"gemma_factual\", \"gemma_Reasoningdepth\", \"gemma_Reasoningtype\", \"gemma_relevance\", \"Gemma_sentence_gemma\"\n",
    "]\n",
    "df = df.iloc[:, :len(cols)]     \n",
    "df.columns = cols              \n",
    "df = df.drop(columns=[\"_drop\"])\n",
    "\n",
    "# =================== RUN JUDGMENT ===================\n",
    "results = []\n",
    "\n",
    "model_to_col_prefix = {\n",
    "    \"llama3.3_70b_sentence\": \"llama\",\n",
    "    \"Mistral24B_sentence\": \"mistral\",\n",
    "    \"Gemma_sentence_gemma\": \"gemma\"\n",
    "}\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    table = row[\"table_content\"]\n",
    "    for model_key in [\"llama3.3_70b_sentence\", \"Mistral24B_sentence\", \"Gemma_sentence_gemma\"]:\n",
    "        model_output = row[model_key]\n",
    "        prompt = build_prompt(table, model_output)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **input_ids,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.0,\n",
    "                do_sample=False,\n",
    "            )\n",
    "\n",
    "        gen_text = tokenizer.decode(output[0][input_ids[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        scores = extract_structured_scores(gen_text)\n",
    "\n",
    "        # human/model reference columns\n",
    "        prefix = model_to_col_prefix[model_key]\n",
    "        factual_col = f\"{prefix}_factual\"\n",
    "        reasoningdepth_col = f\"{prefix}_Reasoningdepth\"\n",
    "        reasoningtype_col = f\"{prefix}_Reasoningtype\"\n",
    "        relevance_col = f\"{prefix}_relevance\"\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"model\": model_key.replace(\"_output\", \"\"),\n",
    "            \"llm_output\": model_output,\n",
    "            \"llm_judge_scores\": scores,\n",
    "            \"raw_response\": gen_text,\n",
    "            \"original_by_human_scores\": {\n",
    "                \"factual\": row.get(factual_col, \"\"),\n",
    "                \"reasoningdepth\": row.get(reasoningdepth_col, \"\"),\n",
    "                \"reasoningtype\": row.get(reasoningtype_col, \"\"),\n",
    "                \"relevance\": row.get(relevance_col, \"\")\n",
    "            }\n",
    "        })\n",
    "\n",
    "# =================== SAVE ===================\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(results)} judgments to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
